{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9524cdb-ed6f-427a-a8f4-a978c63c2f02",
   "metadata": {},
   "source": [
    "## Feature Preprocessing & Model Analysis\n",
    "\n",
    "### 1. Preprocessing & Model Pipelines\n",
    "\n",
    "- **Preprocessing Pipeline**: the pipeline, facilitated by `ColumnTransformer`, was designed to optimize the dataset for subsequent modeling.\n",
    "    - **SimpleImputer**: Missing data points were adeptly handled, ensuring no inadvertent omissions due to partial records.\n",
    "    - **OneHotEncoder**: With the goal of capturing the nuances of categorical variables, categorical attributes were transformed to a machine-learning amicable format.\n",
    "    - **FeaturesTransformer**: An indispensable part of our workflow, the `FeaturesTransformer` seamlessly amalgamated data from various external files like `client_profile_data`, `applications_history_data`, `bki_data`, and `payments_data`. Its engineering prowess manifested in the generation of sophisticated statistics and intricate feature interactions using operations such as multiplication, division, and aggregations.\n",
    "\n",
    "- **Model Pipeline**: Beyond the standard algorithm application, models hyperparameter were via `RandomizedSearchCV` and `BayesSearchCV`, with early stopping acting as a bulwark against overfitting.\n",
    "\n",
    "### 2. Model Performance\n",
    "\n",
    "**Breakdown of Scores across Train, Validation, and Test phases**:\n",
    "\n",
    "- **L2 Model (Linear Regression with L2 Regularization)**\n",
    "    - Train: 0.715\n",
    "    - Validation: 0.71\n",
    "    - Test: 0.731\n",
    "\n",
    "- **Random Forest**\n",
    "    - Train: 0.734\n",
    "    - Validation: 0.699\n",
    "    - Test: 0.731\n",
    "\n",
    "- **XGBoost**\n",
    "    - Train: 0.768\n",
    "    - Validation: 0.727\n",
    "    - Test: 0.736\n",
    "\n",
    "- **LightGBM**\n",
    "    - Train: 0.797\n",
    "    - Validation: 0.729\n",
    "    - Test: 0.757\n",
    "\n",
    "- **CatBoost**\n",
    "    - Train: 0.751\n",
    "    - Validation: 0.724\n",
    "    - Test: 0.743\n",
    "\n",
    "Analyzing the scores, LightGBM's dominance was evident. While some models like LightGBM and XGBoost exhibited minor overfitting tendencies, the depth of their performance was undeniable.\n",
    "\n",
    "### 3. Feature Importance Analysis\n",
    "\n",
    "Post initial observations, a deeper dive into feature importance was undertaken for our top trio: LightGBM, XGBoost, and CatBoost using permutation importance. The subsequent training phase, armed with the most influential features, exhibited the following scores:\n",
    "\n",
    "- **XGBoost**:\n",
    "    - All Features: 0.736\n",
    "    - Selected Features: 0.736\n",
    "\n",
    "- **LightGBM**:\n",
    "    - All Features: 0.757\n",
    "    - Selected Features: 0.753\n",
    "\n",
    "- **CatBoost**:\n",
    "    - All Features: 0.743\n",
    "    - Selected Features: 0.741\n",
    "\n",
    "The performance of models on pruned features revealed an enhanced validation score, confirming our suspicion that a judicious selection can effectively exclude irrelevant attributes. However, the decision gravitated towards employing LightGBM with all its features, given the minor score variations and the importance of retaining comprehensive data narratives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94a9194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder \n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier, early_stopping\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "from tqdm import tqdm\n",
    "import joblib\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "652e5f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    -------\n",
    "    Clean up data, create new features for every dataset and merge everything into one dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - client_profile_data: pandas.core.frame.DataFrame, client profile data\n",
    "    - history_data: pandas.core.frame.DataFrame, applications history data\n",
    "    - bki_data: pandas.core.frame.DataFrame, BKI data\n",
    "    - payments_data: pandas.core.frame.DataFrame, payments data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, client_profile_data, applications_history_data, bki_data, payments_data, percentile_lower=5, percentile_upper=95):\n",
    "            self.client_profile_data = client_profile_data\n",
    "            self.applications_history_data = applications_history_data\n",
    "            self.bki_data = bki_data\n",
    "            self.payments_data = payments_data\n",
    "            self.columns_ = None\n",
    "            self.percentile_lower = percentile_lower\n",
    "            self.percentile_upper = percentile_upper\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "        Fit the transformer to the data and calculate various statistics.\n",
    "\n",
    "        Parameters:\n",
    "        - X: pandas.core.frame.DataFrame, input data\n",
    "        - y: None, not used\n",
    "\n",
    "        Returns:\n",
    "        - self\n",
    "        \"\"\"\n",
    "        # Merging data to create statistics\n",
    "        merged_data = self.merge_tables_on_app_num(X, self.client_profile_data)\n",
    "\n",
    "        # Generate statistics from merged_data\n",
    "        self.set_groupby_profile_stats(merged_data)\n",
    "        self.set_percentile_bonds(merged_data) \n",
    "\n",
    "        # Clearing the temporary merged data\n",
    "        del merged_data\n",
    "        \n",
    "        return self       \n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "        Transform the input data by creating new features and merging datasets.\n",
    "\n",
    "        Parameters:\n",
    "        - X: pandas.core.frame.DataFrame, input data\n",
    "        - y: None, not used\n",
    "\n",
    "        Returns:\n",
    "        - X_transformed: pandas.core.frame.DataFrame, transformed data\n",
    "        \"\"\"\n",
    "        \n",
    "        X = self.create_client_profile_features(X)\n",
    "        history = self.create_applications_history_features(self.applications_history_data)\n",
    "        bki = self.create_bki_features(self.bki_data)\n",
    "        payments = self.create_payments_features(self.payments_data)\n",
    "        \n",
    "        # Merge all Datasets/Tables to one dataset\n",
    "        for table in [history, bki, payments]:\n",
    "            X = self.merge_tables_on_app_num(X, table)\n",
    "            \n",
    "        self.test_ids = list(X[\"APPLICATION_NUMBER\"]) \n",
    "        X.drop(columns=[\"APPLICATION_NUMBER\"], inplace=True)\n",
    "        self.columns_ = X.columns.tolist()\n",
    "            \n",
    "        return X\n",
    "\n",
    "    def get_columns():\n",
    "        return self.columns\n",
    "\n",
    "    def merge_tables_on_app_num(self, left_df, right_df):     \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Merge any two tables based on APPLICATION_NUMBER column\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            left_df: pandas.core.frame.DataFrame\n",
    "            \n",
    "            right_df: pandas.core.frame.DataFrame\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "            merged_df: pandas.core.frame.DataFrame\n",
    "\n",
    "        \"\"\"  \n",
    "\n",
    "        df = pd.merge(left_df, right_df, how=\"left\", on=\"APPLICATION_NUMBER\")\n",
    "        df = df.replace(np.inf, np.nan)\n",
    "        df = df.replace(-np.inf, np.nan)\n",
    "            \n",
    "        return df\n",
    "        \n",
    "    def set_groupby_profile_stats(self, X):     \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Calculate group statists such as TOTAL_SALARY and AMOUNT_CREDIT mean for \n",
    "            EDUCATION_LEVEL, GENDER, FAMILY_STATUS and REGION_POPULATION on the train set\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            X: pandas.core.frame.DataFrame\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        aggs = {\"TOTAL_SALARY\": [\"mean\"], \"AMOUNT_CREDIT\": [\"mean\"]}\n",
    "        self.stats_education = self.create_numerical_aggs(X, groupby_id=\"EDUCATION_LEVEL\", aggs=aggs, prefix=\"GROUPBY_EDU_LEVEL_\")\n",
    "        self.stats_gender = self.create_numerical_aggs(X, groupby_id=\"GENDER\", aggs=aggs, prefix=\"GROUPBY_GENDER_\")\n",
    "        self.stats_region = self.create_numerical_aggs(X, groupby_id=\"REGION_POPULATION\", aggs=aggs, prefix=\"GROUPBY_REGION_\")\n",
    "        self.stats_fam_status = self.create_numerical_aggs(X, groupby_id=\"FAMILY_STATUS\", aggs=aggs, prefix=\"GROUPBY_FAM_STATUS_\")\n",
    "\n",
    "    def set_percentile_bonds(self, X):     \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Calculate left and right percentile bonds for \n",
    "            TOTAL_SALARY, AMOUNT_CREDIT and AMOUNT_ANNUITY on the train set\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            X: pandas.core.frame.DataFrame\n",
    "\n",
    "        \"\"\" \n",
    "        \n",
    "        self.salary_left_bond, self.salary_right_bond = np.nanpercentile(X['TOTAL_SALARY'], q=self.percentile_lower), np.nanpercentile(X['TOTAL_SALARY'], q=self.percentile_upper)\n",
    "        self.credit_left_bond, self.credit_right_bond = np.nanpercentile(X['AMOUNT_CREDIT'], q=self.percentile_lower), np.nanpercentile(X['AMOUNT_CREDIT'], q=self.percentile_upper)\n",
    "        self.annuity_left_bond, self.annuity_right_bond = np.nanpercentile(X['AMOUNT_ANNUITY'], q=self.percentile_lower), np.nanpercentile(X['AMOUNT_ANNUITY'], q=self.percentile_upper)\n",
    "\n",
    "        \n",
    "    def create_numerical_aggs(self, data: pd.DataFrame, groupby_id: str, aggs: dict,\n",
    "                          prefix: Optional[str] = None, suffix: Optional[str] = None,) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Create aggregations for numeric features\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: pandas.core.frame.DataFrame\n",
    "\n",
    "            groupby_id: str\n",
    "\n",
    "            aggs: dict \n",
    "                Dictionary with feature's name and the list of aggr fucntions to perform\n",
    "\n",
    "            prefix: str, optional, default = None\n",
    "                Prefix which will be used to name a new created feature\n",
    "\n",
    "            suffix: str, optional, default = None\n",
    "                Suffix which will be used to name a new created feature\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            stats: pandas.core.frame.DataFrame\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        if not prefix:\n",
    "            prefix = \"\"\n",
    "        if not suffix:\n",
    "            suffix = \"\"\n",
    "\n",
    "        data_grouped = data.groupby(groupby_id)\n",
    "        stats = data_grouped.agg(aggs)\n",
    "        stats.columns = [f\"{prefix}{feature}_{stat}{suffix}\".upper() for feature, stat in stats]\n",
    "        stats = stats.reset_index()\n",
    "\n",
    "        return stats\n",
    "    \n",
    "    def create_client_profile_features(self, X):    \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Create new features for client_profile dataset\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "            X: pandas.core.frame.DataFrame\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            X_transformed: pandas.core.frame.DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        X = X.copy() \n",
    "        X = self.merge_tables_on_app_num(X, self.client_profile_data)\n",
    "  \n",
    "        #FLAG MISSING and OUTLIERS\n",
    "        \n",
    "        # Create new columns to flag features with a lot of missing values\n",
    "        flag_missing_columns = ['OWN_CAR_AGE', 'EXTERNAL_SCORING_RATING_1', 'EXTERNAL_SCORING_RATING_3', 'AMT_REQ_CREDIT_BUREAU_MON']\n",
    "        for column in flag_missing_columns:\n",
    "            X['MISSING_' + column] = X[column].isna().astype('int')\n",
    "\n",
    "        # Flag DAYS_ON_LAST_JOB > 350000 as missing \n",
    "        X['MISSING_DAYS_ON_LAST_JOB'] = (X.DAYS_ON_LAST_JOB > 350000).astype('int')\n",
    "        \n",
    "        # Fill out missing values for MISSING_OWN_CAR_AGE feature as 0\n",
    "        X.loc[X['MISSING_OWN_CAR_AGE']==1,'MISSING_OWN_CAR_AGE'] = 0\n",
    "\n",
    "        #Flag outliers for 'TOTAL_SALARY', 'AMOUNT_CREDIT', 'AMOUNT_ANNUITY'\n",
    "        X['OUTLIER_TOTAL_SALARY'] = ((X['TOTAL_SALARY'] < self.salary_left_bond) | (X['TOTAL_SALARY'] > self.salary_right_bond)).astype('int')\n",
    "        X['OUTLIER_AMOUNT_CREDIT'] = ((X['AMOUNT_CREDIT'] < self.credit_left_bond) | (X['AMOUNT_CREDIT'] > self.credit_right_bond)).astype('int')\n",
    "        X['OUTLIER_AMOUNT_ANNUITY'] = ((X['AMOUNT_ANNUITY'] < self.annuity_left_bond) | (X['AMOUNT_ANNUITY'] > self.annuity_right_bond)).astype('int')\n",
    "\n",
    "        #PROCESS NUMERIC FEATURES\n",
    "        \n",
    "        # Make CHILDRENS as a descrete/categorical feature\n",
    "        X['CHILDREN_0']  = (X.CHILDRENS == 0).astype('int')\n",
    "        X['CHILDREN_1_2'] = ((X['CHILDRENS'] >= 1) & (X['CHILDRENS'] <= 2)).astype('int')\n",
    "        X['CHILDREN_3+']  = (X.CHILDRENS >= 3).astype('int')\n",
    "\n",
    "        # Make FAMILY_SIZE as a descrete/categorical feature\n",
    "        X['FAMILY_SIZE_0']  = (X.FAMILY_SIZE == 0).astype('int')\n",
    "        X['FAMILY_SIZE_1']  = (X.FAMILY_SIZE == 1).astype('int')\n",
    "        X['FAMILY_SIZE_2']  = (X.FAMILY_SIZE == 2).astype('int')\n",
    "        X['FAMILY_SIZE_3+']  = (X.FAMILY_SIZE >= 3).astype('int')\n",
    "        \n",
    "        #Generate new EDUCATION_LEVEL metrics\n",
    "        X = X.merge(self.stats_education, how=\"left\", on=\"EDUCATION_LEVEL\")\n",
    "        X[\"RATIO_CREDIT_to_MEAN_CREDIT_BY_EDUCATION\"] = X[\"AMOUNT_CREDIT\"] / X[\"GROUPBY_EDU_LEVEL_AMOUNT_CREDIT_MEAN\"]\n",
    "        X[\"RATIO_SALARY_to_MEAN_SALARY_BY_EDUCATION\"] = X[\"TOTAL_SALARY\"] / X[\"GROUPBY_EDU_LEVEL_TOTAL_SALARY_MEAN\"]\n",
    "        X[\"DIFF_SALARY_and_MEAN_SALARY_BY_EDUCATION\"] = X[\"TOTAL_SALARY\"] - X[\"GROUPBY_EDU_LEVEL_TOTAL_SALARY_MEAN\"]       \n",
    "\n",
    "        #Generate new GENDER metrics\n",
    "        X = X.merge(self.stats_gender, how=\"left\", on=\"GENDER\")\n",
    "        X[\"RATIO_CREDIT_to_MEAN_CREDIT_BY_GENDER\"] = X[\"AMOUNT_CREDIT\"] / X[\"GROUPBY_GENDER_AMOUNT_CREDIT_MEAN\"]\n",
    "        X[\"RATIO_SALARY_to_MEAN_SALARY_BY_GENDER\"] = X[\"TOTAL_SALARY\"] / X[\"GROUPBY_GENDER_TOTAL_SALARY_MEAN\"]\n",
    "        X[\"DIFF_SALARY_and_MEAN_SALARY_BY_GENDER\"] = X[\"TOTAL_SALARY\"] - X[\"GROUPBY_GENDER_TOTAL_SALARY_MEAN\"]\n",
    "\n",
    "        #Generate new REGION_POPULATION metrics\n",
    "        X = X.merge(self.stats_region, how=\"left\", on=\"REGION_POPULATION\")\n",
    "        X[\"RATIO_CREDIT_to_MEAN_CREDIT_BY_REGION\"] = X[\"AMOUNT_CREDIT\"] / X[\"GROUPBY_REGION_AMOUNT_CREDIT_MEAN\"]\n",
    "        X[\"RATIO_SALARY_to_MEAN_SALARY_BY_REGION\"] = X[\"TOTAL_SALARY\"] / X[\"GROUPBY_REGION_TOTAL_SALARY_MEAN\"]\n",
    "        X[\"DIFF_SALARY_and_MEAN_SALARY_BY_REGION\"] = X[\"TOTAL_SALARY\"] - X[\"GROUPBY_REGION_TOTAL_SALARY_MEAN\"]\n",
    "\n",
    "        #Generate new FAMILY_STATUS metrics\n",
    "        X = X.merge(self.stats_fam_status, how=\"left\", on=\"FAMILY_STATUS\")\n",
    "        X[\"RATIO_CREDIT_to_MEAN_CREDIT_BY_FAM_STATUS\"] = X[\"AMOUNT_CREDIT\"] / X[\"GROUPBY_FAM_STATUS_AMOUNT_CREDIT_MEAN\"]\n",
    "        X[\"RATIO_SALARY_to_MEAN_SALARY_BY_FAM_STATUS\"] = X[\"TOTAL_SALARY\"] / X[\"GROUPBY_FAM_STATUS_TOTAL_SALARY_MEAN\"]\n",
    "        X[\"DIFF_SALARY_and_MEAN_SALARY_BY_FAM_STATUS\"] = X[\"TOTAL_SALARY\"] - X[\"GROUPBY_FAM_STATUS_TOTAL_SALARY_MEAN\"]\n",
    "\n",
    "        # Generate financial metrics\n",
    "        X['RATIO_CREDIT_to_ANNUITY'] = X['AMOUNT_CREDIT'] / X['AMOUNT_ANNUITY'] \n",
    "        X['RATIO_CREDIT_to_SALARY'] = X['AMOUNT_CREDIT'] / X['TOTAL_SALARY'] \n",
    "        X['RATIO_SALARY_TO_CREDIT'] = X['TOTAL_SALARY'] / X['AMOUNT_CREDIT'] \n",
    "        X['RATIO_ANNUITY_to_SALARY'] = X['AMOUNT_ANNUITY'] / X['TOTAL_SALARY'] \n",
    "        X['DIFF_SALARY_and_ANNUITY'] = X['TOTAL_SALARY'] - X['AMOUNT_ANNUITY'] \n",
    "        X[\"FLG_MORE_THAN_50PERCENT_FOR_CREDIT\"] = np.where(X[\"RATIO_ANNUITY_to_SALARY\"] > 0.5, 1, 0)\n",
    "        X[\"FLG_MORE_THAN_30PERCENT_FOR_CREDIT\"] = np.where(X[\"RATIO_ANNUITY_to_SALARY\"] > 0.3, 1, 0)\n",
    "        X[\"FLG_PHONE_and_EMAIL\"] = np.where((X[\"FLAG_PHONE\"]==1)&(X[\"FLAG_EMAIL\"]==1), 1, 0)\n",
    "\n",
    "       # Generate scoring metrics\n",
    "        for function_name in [\"mean\", \"nanmedian\", 'min', 'max', 'var']:\n",
    "            feature_name = \"EXTERNAL_SCORING_{}\".format(function_name)\n",
    "            X[feature_name] = eval(\"np.{}\".format(function_name))(\n",
    "                X[[\"EXTERNAL_SCORING_RATING_1\", \"EXTERNAL_SCORING_RATING_2\", \"EXTERNAL_SCORING_RATING_3\"]], axis=1\n",
    "            )\n",
    "        X[\"EXTERNAL_SCORING_PROD\"] = X[\"EXTERNAL_SCORING_RATING_1\"] * X[\"EXTERNAL_SCORING_RATING_2\"] * X[\"EXTERNAL_SCORING_RATING_3\"]\n",
    "        X[\"EXTERNAL_SCORING_WEIGHTED\"] = X[\"EXTERNAL_SCORING_RATING_1\"] * 2 + X[\"EXTERNAL_SCORING_RATING_2\"] * 1 + X[\"EXTERNAL_SCORING_RATING_3\"] * 3\n",
    "        X[\"EXPECTED_TOTAL_LOSS_1\"] = X[\"EXTERNAL_SCORING_RATING_1\"] * X[\"AMOUNT_CREDIT\"]\n",
    "        X[\"EXPECTED_TOTAL_LOSS_2\"] = X[\"EXTERNAL_SCORING_RATING_2\"] * X[\"AMOUNT_CREDIT\"]\n",
    "        X[\"EXPECTED_TOTAL_LOSS_3\"] = X[\"EXTERNAL_SCORING_RATING_3\"] * X[\"AMOUNT_CREDIT\"]\n",
    "        X[\"EXPECTED_MONTHLY_LOSS_1\"] = X[\"EXTERNAL_SCORING_RATING_1\"] * X[\"AMOUNT_ANNUITY\"]\n",
    "        X[\"EXPECTED_MONTHLY_LOSS_2\"] = X[\"EXTERNAL_SCORING_RATING_2\"] * X[\"AMOUNT_ANNUITY\"]\n",
    "        X[\"EXPECTED_MONTHLY_LOSS_3\"] = X[\"EXTERNAL_SCORING_RATING_3\"] * X[\"AMOUNT_ANNUITY\"]\n",
    "\n",
    "        # Ratio with Age\n",
    "        X[\"RATIO_ANNUITY_to_AGE\"] = X[\"AMOUNT_ANNUITY\"] / X[\"AGE\"]\n",
    "        X[\"RATIO_CREDIT_to_AGE\"] = X[\"AMOUNT_CREDIT\"] / X[\"AGE\"]\n",
    "        X[\"RATIO_SALARY_to_AGE\"] = X[\"TOTAL_SALARY\"] / X[\"AGE\"]\n",
    "        X[\"RATIO_AGE_to_SALARY\"] = X[\"AGE\"] /X[\"TOTAL_SALARY\"]\n",
    "\n",
    "        # Ratio with days_on_last_job\n",
    "        X[\"RATIO_ANNUITY_to_DAYS_ON_LAST_JOB\"] = X[\"AMOUNT_ANNUITY\"] / X[\"DAYS_ON_LAST_JOB\"]\n",
    "        X[\"RATIO_CREDIT_to_DAYS_ON_LAST_JOB\"] = X[\"AMOUNT_CREDIT\"] / X[\"DAYS_ON_LAST_JOB\"]\n",
    "        X[\"RATIO_SALARY_to_DAYS_ON_LAST_JOB\"] = X[\"TOTAL_SALARY\"] / X[\"DAYS_ON_LAST_JOB\"]\n",
    "        X[\"RATIO_DAYS_ON_LAST_JOB_to_SALARY\"] = X[\"DAYS_ON_LAST_JOB\"] /X[\"TOTAL_SALARY\"]\n",
    "        X[\"RATIO_AGE_to_DAYS_ON_LAST_JOB\"] = X[\"AGE\"] /X[\"DAYS_ON_LAST_JOB\"]\n",
    "        X[\"RATIO_AGE_to_OWN_CAR_AGE\"] = X[\"AGE\"] /X[\"OWN_CAR_AGE\"]\n",
    "\n",
    "        # Ratio with FAMILY_SIZE\n",
    "        X[\"RATIO_SALARY_TO_PER_FAMILY_SIZE\"] = X[\"TOTAL_SALARY\"] / X[\"FAMILY_SIZE\"]\n",
    "\n",
    "        #BKI metrics\n",
    "        bki_flags = [flag for flag in X.columns if \"AMT_REQ_CREDIT_BUREAU\" in flag]\n",
    "        X[\"BKI_REQUESTS_COUNT\"] = X[bki_flags].sum(axis=1)\n",
    "        X[\"BKI_KURTOSIS\"] = X[bki_flags].kurtosis(axis=1)\n",
    "        \n",
    "        #Categorical metrics\n",
    "        X.GENDER.replace('XNA', 'Missing', inplace=True)\n",
    "        X.FAMILY_STATUS.replace('Unknown', 'Missing', inplace=True)\n",
    "                              \n",
    "        X = X.drop([\"CHILDRENS\", \"FAMILY_SIZE\"], axis=1)\n",
    " \n",
    "        return X\n",
    "       \n",
    "    def create_applications_history_features(self, X):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Create new features for applications_history_data dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            X_transformed: pandas.core.frame.DataFrame\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create new features for previosly refused applications\n",
    "        aggs_refused = {\n",
    "            'PREV_APPLICATION_NUMBER': ['count'],\n",
    "            'AMT_APPLICATION': ['mean'],\n",
    "            'DAYS_DECISION': ['mean']\n",
    "        }\n",
    "        mask_refused = X[\"NAME_CONTRACT_STATUS\"] == \"Refused\"\n",
    "        stats_refused = self.create_numerical_aggs(X[mask_refused], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_refused, prefix=\"PREV_REFUSED_\")\n",
    "        \n",
    "        # Create new features for previosly approved applications\n",
    "        aggs_approved = {\n",
    "            'PREV_APPLICATION_NUMBER': ['count'],\n",
    "            'AMOUNT_CREDIT': ['sum', 'mean'],\n",
    "        }\n",
    "        mask_approved = X[\"NAME_CONTRACT_STATUS\"] == \"Approved\"\n",
    "        stats_approved = self.create_numerical_aggs(X[mask_approved], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_approved, prefix=\"PREV_APPROVED_\")\n",
    "\n",
    "        # Caution: Ensure that APPLICATION_NUMBER is unique in both datasets to prevent many-to-many merges\n",
    "        res = stats_refused.merge(stats_approved, how='outer', on='APPLICATION_NUMBER')\n",
    "        \n",
    "        return res\n",
    "    \n",
    "    def create_bki_features(self, X):      \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Create new features for bki dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            X_transformed: pandas.core.frame.DataFrame\n",
    "        \"\"\"\n",
    "            \n",
    "        # Create new features for active applications    \n",
    "        aggs_active = {\n",
    "            'CREDIT_DAY_OVERDUE': ['min', 'max', 'mean'],\n",
    "            'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_MAX_OVERDUE': ['mean']\n",
    "        }\n",
    "        mask_active = X['CREDIT_ACTIVE'] =='Active'\n",
    "        stats_active = self.create_numerical_aggs(X[mask_active], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_active, prefix=\"BKI_ACTIVE_\")\n",
    "\n",
    "        # Create new features for closed applications \n",
    "        aggs_closed = {\n",
    "            'CREDIT_DAY_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "            'AMT_CREDIT_MAX_OVERDUE': ['mean']\n",
    "        }\n",
    "        mask_closed = X['CREDIT_ACTIVE'] =='Closed'\n",
    "        stats_closed = self.create_numerical_aggs(X[mask_closed], groupby_id=\"APPLICATION_NUMBER\", aggs=aggs_closed, prefix=\"BKI_CLOSED_\")\n",
    "\n",
    "        # Caution: Ensure that APPLICATION_NUMBER is unique in both datasets to prevent many-to-many merges\n",
    "        res = stats_active.merge(stats_closed, how='outer', on='APPLICATION_NUMBER')\n",
    "        \n",
    "        return res\n",
    " \n",
    "    def create_payments_features(self, X):  \n",
    "        \"\"\"\n",
    "        Description:\n",
    "        -------\n",
    "            Create new features for payment dataset\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            X_transformed: pandas.core.frame.DataFrame\n",
    "        \"\"\"\n",
    "            \n",
    "        X[\"RATIO_DAYS_PAYMENT_to_DAYS_INSTALMENT\"] = X[\"DAYS_ENTRY_PAYMENT\"] / X[\"DAYS_INSTALMENT\"]\n",
    "        X[\"RATIO_DAYS_INSTALMENT_to_DAYS_PAYMENT\"] = X[\"DAYS_INSTALMENT\"] / X[\"DAYS_ENTRY_PAYMENT\"]\n",
    "        X[\"DIFF_DAYS_PAYMENT_and_DAYS_INSTALMENT\"] = X[\"DAYS_ENTRY_PAYMENT\"] - X[\"DAYS_INSTALMENT\"]\n",
    "        X[\"RATIO_AMT_INSTALMENT_to_AMT_PAYMENT\"] = X[\"AMT_INSTALMENT\"] / X[\"AMT_PAYMENT\"]\n",
    "        X[\"RATIO_AMT_PAYMENT_to_AMT_INSTALMENT\"] = X[\"AMT_PAYMENT\"] / X[\"AMT_INSTALMENT\"]\n",
    "        X[\"DIFF_AMT_PAYMENT_and_AMT_INSTALMENT\"] = X[\"AMT_PAYMENT\"] - X[\"AMT_INSTALMENT\"]\n",
    "        X[\"RATIO_DAYS_PAYMENT_to_AMT_PAYMENT\"] = X[\"DAYS_ENTRY_PAYMENT\"] / X[\"AMT_PAYMENT\"]\n",
    "        X[\"RATIO_DAYS_INSTALMENT_to_AMT_INSTALMENT\"] = X[\"DAYS_INSTALMENT\"] / X[\"AMT_INSTALMENT\"]\n",
    "\n",
    "        aggs = {\n",
    "            \"AMT_PAYMENT\": [\"mean\"],\n",
    "            \"AMT_INSTALMENT\": [\"mean\"],\n",
    "            \"RATIO_DAYS_PAYMENT_to_DAYS_INSTALMENT\": [\"mean\", \"std\"],\n",
    "            \"RATIO_DAYS_INSTALMENT_to_DAYS_PAYMENT\": [\"mean\", \"std\"],\n",
    "            \"DIFF_DAYS_PAYMENT_and_DAYS_INSTALMENT\": [\"mean\"],\n",
    "            \"RATIO_AMT_INSTALMENT_to_AMT_PAYMENT\": [\"mean\", \"std\"],\n",
    "            \"DIFF_AMT_PAYMENT_and_AMT_INSTALMENT\": [\"mean\"],\n",
    "            \"RATIO_DAYS_PAYMENT_to_AMT_PAYMENT\": [\"mean\"],\n",
    "            \"RATIO_DAYS_INSTALMENT_to_AMT_INSTALMENT\": [\"mean\", \"std\"],\n",
    "            \"RATIO_AMT_PAYMENT_to_AMT_INSTALMENT\": [\"mean\", \"std\"]\n",
    "        } \n",
    "\n",
    "        res = self.create_numerical_aggs(\n",
    "            X, groupby_id=\"APPLICATION_NUMBER\", aggs=aggs, prefix=\"PAYMENT_STAT_\"\n",
    "        )\n",
    "        \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9494687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_most_important_features(estimator, X_valid, y_valid, threshold=0.0001, metric='roc_auc'):\n",
    "    \"\"\"\n",
    "    Select the most important features based on permutation importance.\n",
    "\n",
    "    Parameters:\n",
    "    - estimator: sklearn-API estimator, model\n",
    "    - X_valid: pandas.core.frame.DataFrame, validation data features\n",
    "    - y_valid: pandas.core.frame.Series, validation data target\n",
    "    - threshold: float, optional, default=0.0001, the threshold for feature importance\n",
    "\n",
    "    Returns:\n",
    "    - best_features: list, names of the best features\n",
    "    \"\"\"\n",
    "    importances = permutation_importance(estimator, X_valid, y_valid, scoring=metric)\n",
    "    important_feature_indices = importances.importances_mean > threshold\n",
    "    best_features = X_valid.columns[important_feature_indices].tolist()\n",
    "\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4d2df3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(estimators, name, test_df, test_ids, scores):\n",
    "\n",
    "    \"\"\"\n",
    "    Description\n",
    "    ----------\n",
    "        Predict on the test dataset and save results as a csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        estimators: callable\n",
    "\n",
    "        test_df: test dataframe\n",
    "        \n",
    "        test_ids: APPLICATION_NUMBER for test dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        pred: pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    pred = estimators.predict_proba(test_df)[:, 1]\n",
    "    results = pd.DataFrame({\n",
    "        \"APPLICATION_NUMBER\": test_ids,\n",
    "        \"TARGET\": pred\n",
    "    })   \n",
    "    fn = f\"../data/results/results_{name}_{scores}.csv\"\n",
    "    print(f\"Result succesfullty saved to {fn}\")\n",
    "    results.to_csv(fn, index=False)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21d2a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_scores_roc_auc(name, model, X_train, X_valid, X_test, y_train, y_valid, y_test, test):\n",
    "    \n",
    "    # Make a predicition on different validation train datasets\n",
    "    pred_train = model.predict_proba(X_train)\n",
    "    pred_valid = model.predict_proba(X_valid)\n",
    "    pred_test = model.predict_proba(X_test)\n",
    "    \n",
    "    # Score roc_auc\n",
    "    train_score = round(roc_auc_score(y_train, pred_train[:, 1]), 3)\n",
    "    valid_score = round(roc_auc_score(y_valid, pred_valid[:, 1]), 3)\n",
    "    test_score = round(roc_auc_score(y_test, pred_test[:, 1]), 3)\n",
    "\n",
    "    #Count how many observations are marked as 1\n",
    "    test_count_1= (model.predict(test)==1).sum()\n",
    "    \n",
    "    print(\n",
    "        f'Model: {name}, ',\n",
    "        f'Train-score: {train_score}, ',\n",
    "        f'Valid-score: {valid_score}, ',\n",
    "        f'Test-score: {test_score}, ',\n",
    "        f'Target count: {test_count_1}'\n",
    "    )\n",
    "    \n",
    "    return train_score, valid_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d17d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelParam():\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    --------\n",
    "    A class for managing hyperparameter grids and fit parameters for different machine learning models.\n",
    "\n",
    "    Parameters:\n",
    "    - model_name: str, the name of the machine learning model\n",
    "    - X: pandas.core.frame.DataFrame, optional, default=None, input features\n",
    "    - y: pandas.core.frame.Series, optional, default=None, target variable\n",
    "    - model_random_state: int, optional, default=123, random state for data splitting and model initialization\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, model_name, X=None, y=None, X_valid=None, y_valid=None):\n",
    "        self.model_name = model_name\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.hyperparam_grid = self.set_grid_hyperparam()\n",
    "        self.hyperparam_fit = self.set_fit_hyperparam()\n",
    "        \n",
    "    def set_grid_hyperparam(self):\n",
    "        \"\"\"\n",
    "        Set the hyperparameter grid for different machine learning models.\n",
    "\n",
    "        Returns:\n",
    "        - grid_hyperparams: dict, a dictionary containing hyperparameter grids for various models\n",
    "        \"\"\"\n",
    "        \n",
    "        grid_hyperparams = {\n",
    "            'l2': {\n",
    "                'logisticregression__C': [0.001],\n",
    "                'logisticregression__penalty': ['l2'],\n",
    "                'logisticregression__max_iter': [1000]\n",
    "                \n",
    "            },\n",
    "            'rf': {\n",
    "                'randomforestclassifier__n_estimators': [500],\n",
    "                'randomforestclassifier__max_depth': [15],\n",
    "                'randomforestclassifier__max_features': ['sqrt'],\n",
    "                'randomforestclassifier__min_samples_leaf': [500],\n",
    "                'randomforestclassifier__bootstrap': [True],\n",
    "                'randomforestclassifier__criterion': ['gini']\n",
    "            },\n",
    "            'xgb': {\n",
    "                \"xgbclassifier__booster\": [\"gbtree\"],\n",
    "                \"xgbclassifier__objective\": [\"binary:logistic\"],\n",
    "                \"xgbclassifier__eval_metric\": [\"auc\"],\n",
    "                \"xgbclassifier__learning_rate\": [0.1],\n",
    "                \"xgbclassifier__n_estimators\": [10000],\n",
    "                \"xgbclassifier__reg_lambda\": [0.1],\n",
    "                \"xgbclassifier__max_depth\": [3],\n",
    "                \"xgbclassifier__verbosity\": [0],\n",
    "                \"xgbclassifier__early_stopping_rounds\": [100],\n",
    "                \"xgbclassifier__verbosity\": [0],\n",
    "                \"xgbclassifier__colsample_bytree\": [0.3] \n",
    "            },\n",
    "            'lgb': {\n",
    "                \"lgbmclassifier__boosting_type\": [\"gbdt\"],\n",
    "                \"lgbmclassifier__objective\": [\"binary\"],\n",
    "                \"lgbmclassifier__learning_rate\": [0.01],\n",
    "                \"lgbmclassifier__n_estimators\": [1139],\n",
    "                \"lgbmclassifier__verbosity\": [-1],\n",
    "                \"lgbmclassifier__max_depth\": [8],\n",
    "                \"lgbmclassifier__silent\": [True],\n",
    "                \"lgbmclassifier__num_leaves\": [10000],\n",
    "                \"lgbmclassifier__bagging_fraction\": [0.3],\n",
    "                \"lgbmclassifier__min_data_in_leaf\": [1000]\n",
    "            },\n",
    "            'cb': {\n",
    "                \"catboostclassifier__n_estimators\": [4648],\n",
    "                \"catboostclassifier__loss_function\": [\"Logloss\"],\n",
    "                \"catboostclassifier__eval_metric\": [\"AUC\"],\n",
    "                \"catboostclassifier__learning_rate\": [0.06],\n",
    "                \"catboostclassifier__max_bin\": [333],\n",
    "                \"catboostclassifier__verbose\": [0],\n",
    "                \"catboostclassifier__max_depth\": [3],\n",
    "                \"catboostclassifier__l2_leaf_reg\": [71.4],\n",
    "                \"catboostclassifier__early_stopping_rounds\": [100]\n",
    "            }\n",
    "        }\n",
    "                       \n",
    "        return grid_hyperparams.get(self.model_name, \"none\")\n",
    "\n",
    "    def set_fit_hyperparam(self):\n",
    "        \"\"\"\n",
    "        Set the fit parameters and data splits for different machine learning models.\n",
    "\n",
    "        Returns:\n",
    "        - fit_hyperparams: dict, a dictionary containing fit parameters for various models\n",
    "        \"\"\"\n",
    "\n",
    "        X_train, y_train, X_valid, y_valid = self.X, self.y, self.X_valid, self.y_valid\n",
    "        stopper = early_stopping(stopping_rounds=100, first_metric_only=False)\n",
    "        \n",
    "        fit_hyperparams = {\n",
    "            'xgb': {\n",
    "                'xgbclassifier__eval_set': [(X_train, y_train), (X_valid, y_valid)], \n",
    "                'xgbclassifier__verbose': 0\n",
    "            },\n",
    "            'lgb': {\n",
    "                'lgbmclassifier__eval_set': [(X_train, y_train), (X_valid, y_valid)], \n",
    "                \"lgbmclassifier__eval_metric\": \"auc\",\n",
    "                'lgbmclassifier__callbacks': [stopper]\n",
    "            },\n",
    "            'cb': {\n",
    "                'catboostclassifier__eval_set': [(X_train, y_train), (X_valid, y_valid)]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return fit_hyperparams.get(self.model_name, {})   \n",
    "    \n",
    "    def get_grid_hyperparam(self):\n",
    "        \"\"\"\n",
    "        Get the hyperparameter grid for the selected machine learning model.\n",
    "\n",
    "        Returns:\n",
    "        - hyperparam_grid: dict, a dictionary containing hyperparameter grid for the model\n",
    "        \"\"\"\n",
    "        return self.hyperparam_grid\n",
    "    \n",
    "    def get_fit_hyperparam(self):\n",
    "        \"\"\"\n",
    "        Get the fit parameters for the selected machine learning model.\n",
    "\n",
    "        Returns:\n",
    "        - hyperparam_fit: dict, a dictionary containing fit parameters for the model\n",
    "        \"\"\"\n",
    "        return self.hyperparam_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5129588a",
   "metadata": {},
   "source": [
    "## Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27c90dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "applications_history_data = pd.read_csv('../data/applications_history.csv')\n",
    "bki_data = pd.read_csv('../data/bki.csv')\n",
    "client_profile_data = pd.read_csv('../data/client_profile.csv')\n",
    "payments_data = pd.read_csv('../data/payments.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "train_data = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "913dc56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_data.drop(columns=[\"TARGET\"]), test_data.copy()\n",
    "target = train_data[\"TARGET\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a7341fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train dataset to train, valid and test to do the holdout validation\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    train, target, test_size=0.1, random_state=1234, stratify=target\n",
    ")\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(\n",
    "    X_valid, y_valid, test_size=0.2, random_state=1234, stratify=y_valid\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6c127",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c985ad47-0647-477d-bcfd-fd12b4a22eab",
   "metadata": {},
   "source": [
    "#### NOTE: FeaturesTransformer is not part of the Preprocessing Pipeline as we need to retrieve column names for permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b243434c-6206-4fe2-aa5f-4c1bcbdf60b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the FeaturesTransformer Once and Get Column Names:\n",
    "features_transformer = FeaturesTransformer(client_profile_data, applications_history_data, bki_data, payments_data)\n",
    "X_train_transformed = features_transformer.fit_transform(X_train)\n",
    "all_features = X_train_transformed.columns.tolist()\n",
    "\n",
    "# Divide the columns into numerical and categorical\n",
    "categorical_columns = [col for col in all_features if X_train_transformed[col].dtype == 'O']\n",
    "numerical_columns = [col for col in all_features if col not in categorical_columns]\n",
    "\n",
    "# Create Your Preprocessing Pipeline with ColumnTransformer\n",
    "preprocessing = make_pipeline(\n",
    "    make_column_transformer(\n",
    "        (make_pipeline(SimpleImputer(strategy='constant', fill_value='Missing'), OneHotEncoder(sparse_output=False)), \n",
    "         categorical_columns),\n",
    "        (SimpleImputer(strategy='median'), numerical_columns),\n",
    "        remainder='passthrough'\n",
    "    )\n",
    ")\n",
    "\n",
    "X_train = preprocessing.fit_transform(X_train_transformed)\n",
    "X_valid = preprocessing.transform(features_transformer.transform(X_valid))\n",
    "X_test = preprocessing.transform(features_transformer.transform(X_test))\n",
    "test_no_target = preprocessing.transform(features_transformer.transform(test))\n",
    "\n",
    "# Retrieve the transformers\n",
    "ohe_categories = preprocessing.named_steps['columntransformer'].named_transformers_['pipeline'].named_steps['onehotencoder'].get_feature_names_out(categorical_columns)\n",
    "\n",
    "# Combine the names of the one-hot-encoded columns with the names of the numerical columns\n",
    "all_columns = list(ohe_categories) + numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edf53349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating model pipelines\n",
    "pipelines = {\n",
    "    'l2': make_pipeline(StandardScaler(), LogisticRegression()),\n",
    "    'rf': make_pipeline(RandomForestClassifier()),\n",
    "    'xgb': make_pipeline(XGBClassifier()),\n",
    "    'lgb': make_pipeline(LGBMClassifier()),\n",
    "    'cb': make_pipeline(CatBoostClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2274d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l2 has been fitted.\n",
      "rf has been fitted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb has been fitted.\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[946]\ttraining's auc: 0.796933\ttraining's binary_logloss: 0.23573\tvalid_1's auc: 0.729111\tvalid_1's binary_logloss: 0.255065\n",
      "lgb has been fitted.\n",
      "cb has been fitted.\n"
     ]
    }
   ],
   "source": [
    "# Create empty dictionary called fitted_models\n",
    "fitted_models = {}\n",
    "\n",
    "# Loop through model pipelines, tuning each one and saving it to fitted_models\n",
    "for name, pipeline in pipelines.items():\n",
    "    model_param = ModelParam(name, X_train, y_train, X_valid, y_valid) \n",
    "    model = RandomizedSearchCV(pipeline, model_param.get_grid_hyperparam(), cv=5, n_jobs=-1, verbose=0)\n",
    "    \n",
    "    # Fit model on X_train, y_train\n",
    "    if model_param.get_fit_hyperparam() == 'none':\n",
    "        model.fit(X_train, y_train)\n",
    "    else:\n",
    "        model.fit(X_train, y_train, **model_param.get_fit_hyperparam())\n",
    "\n",
    "   \n",
    "    # Store model in fitted_models[name] \n",
    "    fitted_models[name] = model\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(name, 'has been fitted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5369d752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: l2,  Train-score: 0.715,  Valid-score: 0.71,  Test-score: 0.731,  Target count: 43\n",
      "Result succesfullty saved to ../data/results/results_l2_0.731.csv\n",
      "Model: rf,  Train-score: 0.734,  Valid-score: 0.699,  Test-score: 0.73,  Target count: 0\n",
      "Result succesfullty saved to ../data/results/results_rf_0.73.csv\n",
      "Model: xgb,  Train-score: 0.768,  Valid-score: 0.727,  Test-score: 0.736,  Target count: 379\n",
      "Result succesfullty saved to ../data/results/results_xgb_0.736.csv\n",
      "Model: lgb,  Train-score: 0.797,  Valid-score: 0.729,  Test-score: 0.757,  Target count: 266\n",
      "Result succesfullty saved to ../data/results/results_lgb_0.757.csv\n",
      "Model: cb,  Train-score: 0.751,  Valid-score: 0.724,  Test-score: 0.743,  Target count: 426\n",
      "Result succesfullty saved to ../data/results/results_cb_0.743.csv\n"
     ]
    }
   ],
   "source": [
    "for name, model in fitted_models.items():\n",
    "    _, _, test_score = predict_and_scores_roc_auc(name, model, X_train, X_valid, X_test, y_train, y_valid, y_test, test_no_target)\n",
    "    test_ids_ = features_transformer.test_ids\n",
    "    save_predictions(model, name, test_no_target, test_ids_, test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c6d73",
   "metadata": {},
   "source": [
    "### Best Models with most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57877bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Categorical and Numerical features\n",
    "preprocessing_ = make_pipeline(\n",
    "    FeaturesTransformer(client_profile_data, applications_history_data, bki_data, payments_data),\n",
    "    make_column_transformer(\n",
    "        (make_pipeline(SimpleImputer(strategy='constant', fill_value='Missing'), OneHotEncoder(sparse_output=False)), \n",
    "         make_column_selector(dtype_exclude='number')),\n",
    "        (SimpleImputer(strategy='median'), make_column_selector(dtype_include='number')),\n",
    "        remainder='passthrough'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Function for creating model pipelines\n",
    "b_pipelines = {\n",
    "    'xgb': make_pipeline(XGBClassifier()),\n",
    "    'lgb': make_pipeline(LGBMClassifier()),\n",
    "    'cb': make_pipeline(CatBoostClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff1a2be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb  most important features have been selected\n",
      "lgb  most important features have been selected\n",
      "cb  most important features have been selected\n"
     ]
    }
   ],
   "source": [
    "# Train models with best scores on the most important features\n",
    "best_model_features = {}\n",
    "for model in b_pipelines.keys():\n",
    "    best_model_features[model] = select_most_important_features(fitted_models[model], \n",
    "                                                        pd.DataFrame(X_valid, columns=all_columns), \n",
    "                                                        y_valid)\n",
    "    print(model, ' most important features have been selected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8cea9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:440: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(data):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:312: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(dtype):\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:314: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  elif is_categorical_dtype(dtype) and enable_categorical:\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:345: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if is_categorical_dtype(dtype)\n",
      "/Users/sashas/PycharmProjects/predictLoanRepayment/venv/lib/python3.9/site-packages/xgboost/data.py:336: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  return is_int or is_bool or is_float or is_categorical_dtype(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb has been fitted.\n",
      "Model: xgb,  Train-score: 0.785,  Valid-score: 0.735,  Test-score: 0.736,  Target count: 425\n",
      "Result succesfullty saved to ../data/results/results_xgb_0.736.csv\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1114]\ttraining's auc: 0.800794\ttraining's binary_logloss: 0.234106\tvalid_1's auc: 0.733446\tvalid_1's binary_logloss: 0.254403\n",
      "lgb has been fitted.\n",
      "Model: lgb,  Train-score: 0.801,  Valid-score: 0.733,  Test-score: 0.753,  Target count: 305\n",
      "Result succesfullty saved to ../data/results/results_lgb_0.753.csv\n",
      "cb has been fitted.\n",
      "Model: cb,  Train-score: 0.754,  Valid-score: 0.735,  Test-score: 0.741,  Target count: 477\n",
      "Result succesfullty saved to ../data/results/results_cb_0.741.csv\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[647]\tvalid_0's auc: 0.770056\tvalid_0's binary_logloss: 0.242843\tvalid_1's auc: 0.723304\tvalid_1's binary_logloss: 0.255955\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1035]\tvalid_0's auc: 0.784946\tvalid_0's binary_logloss: 0.238603\tvalid_1's auc: 0.728487\tvalid_1's binary_logloss: 0.255021\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[958]\tvalid_0's auc: 0.781994\tvalid_0's binary_logloss: 0.239118\tvalid_1's auc: 0.728926\tvalid_1's binary_logloss: 0.254885\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[831]\tvalid_0's auc: 0.777921\tvalid_0's binary_logloss: 0.240578\tvalid_1's auc: 0.727202\tvalid_1's binary_logloss: 0.255145\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[918]\tvalid_0's auc: 0.780436\tvalid_0's binary_logloss: 0.239609\tvalid_1's auc: 0.727554\tvalid_1's binary_logloss: 0.255466\n"
     ]
    }
   ],
   "source": [
    "best_fitted_models = {}\n",
    "for b_model, b_pipeline in b_pipelines.items():\n",
    "\n",
    "    #Split data\n",
    "    X_train_, X_valid_, y_train_, y_valid_ = train_test_split(\n",
    "        train, target, test_size=0.1, random_state=1234, stratify=target)\n",
    "    X_valid_, X_test_, y_valid_, y_test_ = train_test_split(\n",
    "        X_valid_, y_valid_, test_size=0.2, random_state=1234, stratify=y_valid_)\n",
    "\n",
    "    # Transform data\n",
    "    X_train_transformed_ = preprocessing_.fit_transform(X_train_)\n",
    "    X_valid_transformed_ = preprocessing_.transform(X_valid_)\n",
    "    X_test_transformed_ = preprocessing_.transform(X_test_)\n",
    "    test_no_target_transformed_ = preprocessing_.transform(test)\n",
    "\n",
    "    #Create trnasfor df with columns\n",
    "    X_train_transformed_ = pd.DataFrame(X_train_transformed_, columns=all_columns)\n",
    "    X_valid_transformed_ = pd.DataFrame(X_valid_transformed_, columns=all_columns)\n",
    "    X_test_transformed_ = pd.DataFrame(X_test_transformed_, columns=all_columns)\n",
    "    test_no_target_transformed_ = pd.DataFrame(test_no_target_transformed_, columns=all_columns)\n",
    "\n",
    "\n",
    "    #Filter data by most important columns\n",
    "    transformed_columns = best_model_features[b_model]\n",
    "    X_train_ = pd.DataFrame(X_train_transformed_, columns=transformed_columns)\n",
    "    X_valid_ = pd.DataFrame(X_valid_transformed_, columns=transformed_columns)\n",
    "    X_test_ = pd.DataFrame(X_test_transformed_, columns=transformed_columns)\n",
    "    test_no_target_ = pd.DataFrame(test_no_target_transformed_, columns=transformed_columns)\n",
    "\n",
    "    # Fit model\n",
    "    model_param_ = ModelParam(b_model, X_train_, y_train_, X_valid_, y_valid_)\n",
    "    model_ = RandomizedSearchCV(b_pipeline, model_param_.get_grid_hyperparam(), cv=5, n_jobs=-1)\n",
    "    model_.fit(X_train_, y_train_, **model_param_.get_fit_hyperparam())\n",
    "\n",
    "    # Store model in fitted_models[name] \n",
    "    best_fitted_models[b_model] = model_\n",
    "    \n",
    "    # Print '{name} has been fitted'\n",
    "    print(b_model, 'has been fitted.')\n",
    "\n",
    "    train_score, valid_score, test_score = predict_and_scores_roc_auc(b_model, model_, X_train_, X_valid_, X_test_, y_train_, y_valid_, y_test_, test_no_target_)\n",
    "    test_ids_ = preprocessing_.named_steps['featurestransformer'].test_ids\n",
    "    save_predictions(model_, b_model, test_no_target_, test_ids_, test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c2a321a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessing pipeling to be used in the script\n",
    "joblib.dump(preprocessing, '../models/preprocessing_pipeline.pkl')\n",
    "\n",
    "# Save Best performed model to be used in the script\n",
    "with open('../models/model.pkl', 'wb') as f:\n",
    "    pickle.dump(fitted_models['lgb'].best_estimator_, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
